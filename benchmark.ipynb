{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt import gpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the prompts for generating facts:\n",
    "number_of_facts = 30\n",
    "number_of_questions = 5\n",
    "prompt = f\"\"\"\n",
    "First, generate {number_of_facts} facts about a character named John and his friends: you can make up the name for his friends on your own. Be sure to be creative with the facts, and generate facts that describe many different aspects of John's life. Number these facts 1 through {number_of_facts} and display them. Surround the facts with a \"[START FACTS]\" and \"[END FACTS]\" label.\n",
    "Then, create {number_of_questions} multiple choice questions revolving around the generated facts that a user who knows John very well should know the answer to, each with 4 choices: one of the choices should be correct, while the other 3 should be incorrect. Surround each question and the choices with a \"[START QUESTION]\" and \"[END QUESTION]\" label, and surround the correct answer with a \"[START ANSWER]\" and \"[END ANSWER]\" label. \n",
    "\"\"\"\n",
    "# response = openai.ChatCompletion.create(\n",
    "#             model='gpt-4',\n",
    "#             messages=[{'role': 'user', 'content': prompt}]\n",
    "#         )\n",
    "# response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "# response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "    response_content = response\n",
    "    try:\n",
    "        # Processing the facts\n",
    "        response_content = response_content.split(\"[END FACTS]\")\n",
    "        facts = response_content[0].split(\"[START FACTS]\")[1]\n",
    "        fact_array = facts.split(\"\\n\")\n",
    "        fact_array = list(filter(None, fact_array))\n",
    "        for i in range(len(fact_array)):\n",
    "            fact_array[i] = fact_array[i].split('.')[1][1:]\n",
    "        \n",
    "        # Processing the questions\n",
    "        questions_array = []\n",
    "        questions = response_content[1].split(\"[END ANSWER]\")\n",
    "        for question in questions[:-1]:\n",
    "            question = question.split(\"[START ANSWER]\")\n",
    "            letter_choice = question[1].strip('\\n')\n",
    "            letter_choice = letter_choice[0]\n",
    "            question = question[0].split(\"[START QUESTION]\")\n",
    "            question = question[1].strip('\\n')\n",
    "            question = question.split(\"[END QUESTION]\")[0]\n",
    "            questions_array.append((question, letter_choice))\n",
    "        \n",
    "        return fact_array, questions_array\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error parsing, moving on...\") \n",
    "        return None, None\n",
    "\n",
    "# fact_array, questions_array = process_response(response)\n",
    "# print(fact_array)\n",
    "# print(questions_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:44<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate Dataset:\n",
    "import json\n",
    "num_points = 100\n",
    "dataset = []\n",
    "responses = gpts([prompt]*num_points, model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [01:17<00:00,  2.57it/s]\n",
      "100%|██████████| 200/200 [01:41<00:00,  1.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generating 400 more datapoints:\n",
    "num_points = 400\n",
    "responses2 = gpts([prompt]*num_points, model='gpt-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = responses + responses2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string index out of range\n",
      "Error parsing, moving on...\n",
      "list index out of range\n",
      "Error parsing, moving on...\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for response in responses: \n",
    "    fact_array, questions_array = process_response(response)\n",
    "    if fact_array and questions_array:\n",
    "        data = {'facts': fact_array, 'questions': questions_array}\n",
    "        dataset.append(data)\n",
    "\n",
    "with open('data/data.json', 'w') as f:\n",
    "    json.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benshi/opt/anaconda3/envs/agenv/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from agent import MemoryAgent\n",
    "from datastore import PineconeDatastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [50:49<00:00, 30.50s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('data/data.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "dataset_part1 = dataset[:100]\n",
    "# Evaluate the baseline (with context)\n",
    "def get_answers(d):\n",
    "    prompts = []\n",
    "    answers = []\n",
    "    for point in d:\n",
    "        fact_array = point['facts']\n",
    "        questions_array = point['questions']\n",
    "        facts_string = '\\n'.join(fact_array)\n",
    "        # Creating the prompts:\n",
    "        prompt = f\"\"\"Here are facts about a person. Given these facts, answer all the given multiple choice question based on the given facts. Respond with just the letter of the correct answer for each question, and nothing else, and separate each answer with a space in between.\n",
    "        [START FACTS]\n",
    "        {facts_string}\n",
    "        [END FACTS]\n",
    "        [START QUESTION]\n",
    "        {questions_array[0][0]}\n",
    "        [END QUESTION]\n",
    "        [START QUESTION]\n",
    "        {questions_array[1][0]}\n",
    "        [END QUESTION]\n",
    "        [START QUESTION]\n",
    "        {questions_array[2][0]}\n",
    "        [END QUESTION]\n",
    "        [START QUESTION]\n",
    "        {questions_array[3][0]}\n",
    "        [END QUESTION]\n",
    "        [START QUESTION]\n",
    "        {questions_array[4][0]}\n",
    "        [END QUESTION]\n",
    "        \"\"\"\n",
    "        prompts.append(prompt)\n",
    "        answers.append(questions_array[0][1])\n",
    "        answers.append(questions_array[1][1])\n",
    "        answers.append(questions_array[2][1])\n",
    "        answers.append(questions_array[3][1])\n",
    "        answers.append(questions_array[4][1])\n",
    "\n",
    "    responses = gpts(prompts, model='gpt-4')\n",
    "    return responses, answers\n",
    "responses1 = get_answers(dataset_part1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'responses1' (list)\n"
     ]
    }
   ],
   "source": [
    "%store responses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.11111111111111\n"
     ]
    }
   ],
   "source": [
    "# Grading:\n",
    "score = 0\n",
    "bad_count = 0\n",
    "for r, point in zip(responses1, dataset[:100]):\n",
    "    model_answers = r.split(' ')\n",
    "    if len(model_answers) != 5:\n",
    "        bad_count += 5\n",
    "        continue\n",
    "    for i, ans in enumerate(model_answers):\n",
    "        if '.' in ans:\n",
    "            model_answers[i] = ans.strip('.')\n",
    "\n",
    "    questions_array = point['questions']\n",
    "    for i, question in enumerate(questions_array):\n",
    "        answer = question[1]\n",
    "        if not answer or answer == '\\n' or answer == ' ':\n",
    "            bad_count += 1\n",
    "            continue\n",
    "        if answer.strip().lower() == model_answers[i].strip().lower():\n",
    "            score += 1\n",
    "        # elif len(answer.strip().strip('\\n').strip().lower()) != len(model_answers[i].strip().strip('\\n').strip().lower()):\n",
    "        #     print(answer.lower())\n",
    "        #     print(model_answers[i].lower())\n",
    "total = 500 - bad_count\n",
    "percentage_score = score / total * 100\n",
    "print(percentage_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "B\n",
      "B\n",
      "B\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "for question in dataset[0]['questions']:\n",
    "    print(question[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_part2 = dataset[100:200]\n",
    "responses2 = get_answers(dataset_part2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C B B B B']\n"
     ]
    }
   ],
   "source": [
    "print(responses1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Agent: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
